module Monitor;

export *;
import * from Architecture;
import * from Param;
import * from ScaleWrapper;

interface MonitorInterface {}

class Monitor(PrometheusInterface prometheus, String serviceName, WrapperInterface wrapper, Int baseN) implements MonitorInterface {
    Rat time = monitoring_window()/time_unit_to_sec();
    Rat monitored_inst = baseN;

    Unit run() {
        while(time <= simulation_duration()) {
            await duration(monitoring_window(), monitoring_window());
            Rat latency = await prometheus!getV("latency", serviceName);
            Rat total_req = await prometheus!getV("total request", serviceName);
            Rat loss = await prometheus!getV("request loss", serviceName);
            Rat completed = await prometheus!getV("completed", serviceName);
            Rat instances = await prometheus!getV("instances", serviceName);
            Rat waiting = await prometheus!getV("waiting", serviceName);
            Rat avg_lat = 0;
            Rat throughput = time_unit_to_sec() * total_req / monitoring_window();
            Rat add_throughput = time_unit_to_sec() * (total_req - completed) / monitoring_window();
            Rat compl_throughput = time_unit_to_sec() * completed / monitoring_window();
            if(completed != 0) avg_lat = latency/completed;
            if(metric_for_scaling() == "latency") {
                if(avg_lat > time_unit_to_sec() * lookupUnsafe(latency_threshold(), serviceName)) wrapper!deploy();
                else if(avg_lat < time_unit_to_sec() * lookupUnsafe(latency_threshold(), serviceName)) wrapper!undeploy();
            }
            else if(metric_for_scaling() == "MCL") {
               Rat mcl = lookupUnsafe(serviceMCLs(), serviceName);
               if(add_throughput > 0) throughput = throughput + add_throughput;
               Int target = ceil(float(throughput/mcl));
            //    if(target > 10) target = 10;
               if(target > instances) this!scaleUp(target - instances);
               else if(target < instances) this!scaleDown(instances - target);
            }    
            // println(serviceName + "[Time: " + toString(time) + " Latency: " + toString(float(avg_lat)) + "s Instances: " + toString(instances) + " Request Throughput: " + toString(float(throughput)) +  " Waiting: " + toString(float(add_throughput)) + " Response Throughput: " + toString(float(compl_throughput)) + "]");
            time = time + (monitoring_window()/time_unit_to_sec());      
        }
    }

    Unit scaleUp(Rat toDeploy) {
        while(toDeploy != 0) {
            wrapper!deploy();
            toDeploy = toDeploy - 1;
        }
    }

    Unit scaleDown(Rat toDeploy) {
        while(toDeploy != 0) {
            wrapper!undeploy();
            toDeploy = toDeploy - 1;
        }
    }
}